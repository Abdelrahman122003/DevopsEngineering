> # Node-local persistent volumes

When using a hostPath volume in a pod, be aware that the data accessible to the pod depends on the node to which the pod is scheduled. If the pod is deleted and recreated on a different node, it may no longer have access to the same data, as the hostPath volume is tied to the specific node's filesystem.

If you use a local persistent volume instead, this problem is resolved. The Kubernetes scheduler ensures that the pod is always scheduled on the node where the local volume is attached.

`NOTE`

Local persistent volumes are also better than hostPath volumes because they offer much better security. As explained in the previous chapter, you don’t want to allow regular users to use hostPath volumes at all. Because persistent volumes are managed by the cluster administrator, regular users can’t use them to access arbitrary paths on the host node.

## Creating local persistent volumes

- Creating a storage class to represent local storage

  ```yaml
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    name: local #A
  provisioner: kubernetes.io/no-provisioner #B
  volumeBindingMode:
    WaitForFirstConsumer #C
    #A Let’s call this storage class local
    #B Persistent volumes of this class are provisioned manually
    #C The persistent volume claim should be bound only when the first pod that uses the claim is deployed.
  ```

  As I write this, locally attached persistent volumes need to be provisioned manually, so you need to set the provisioner as shown in the listing. Because this storage class represents locally attached volumes that can only be accessed within the nodes to which they are physically connected

- Attaching a disk to a cluster node

  I assume that you’re using a Kubernetes cluster created with the kind tool to run this exercise. Let’s emulate the installation of the SSD in the node called kind-worker. Run the following command to create an empty directory at the location /mnt/ssd1 in the node’s filesystem:

  ```shell
  docker exec kind-worker mkdir /mnt/ssd1
  ```

- Creating a PersistentVolume object for the new disk

  After attaching the disk to one of the nodes, you must tell Kubernetes that this node now provides a local persistent volume by creating a PersistentVolume object.

  ```yaml
  kind: PersistentVolume
  apiVersion: v1
  metadata:
    name: local-ssd-on-kind-worker              #A
  spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: local                     #B
  capacity:
      storage: 10Gi
  local:                                      #C
      path: /mnt/ssd1                           #C
  nodeAffinity:                               #D
      required:                                 #D
      nodeSelectorTerms:                      #D
      - matchExpressions:                     #D
          - key: kubernetes.io/hostname         #D
          operator: In                        #D
          values:                             #D
          - kind-worker                       #D
    #A This persistent volume represents the local SSD installed in the kind-worker node, hence the name.
    #B This volume belongs to the local storage class.
    #C This volume is mounted in the node’s filesystem at the specified path.
    #D This section tells Kubernetes which nodes can access this volume. Since the SSD is attached only to the node kind-worker, it is only accessible on this node.
  ```

The persistent volume is named to reflect that it's a local disk attached to the kind-worker node, referencing the local storage class. It represents storage directly attached to the node, with the configuration specifying the mount path at /mnt/ssd1.

The `node affinity` definition in the listing simply defines that the volume is accessible from nodes whose hostname is kind-worker. This is obviously exactly one node.

## Claiming and using local persistent volumes

As an application developer, you can now deploy your pod and its associated
persistent volume claim.

- Creating a pod
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
      name: mongodb-local
  spec:
      volumes:
  - name: mongodb-data
  persistentVolumeClaim:
    claimName: mongodb-pvc-local            #A
  containers:
  - image: mongo
  name: mongodb
  volumeMounts:
  - name: mongodb-data
    mountPath: /data/db
  #A The pod uses the mongodb-pvc-local claim
  ```
- Creating the persistent volume claim for a local volume

  ```yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: mongodb-pvc-local
  spec:
  storageClassName: local #A
  resources:
    requests:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  #A The claim requests a persistent volume from the local storage class
  ```

- Creating the pod and the claim

  After both the pod and the claim are created, the following events take place:

  - The claim is bound to the persistent volume.
  - The scheduler determines that the volume bound to the claim that is used in the pod can only be accessed from the kind-worker node, so it schedules the pod to this node.
  - The pod’s container is started on this node, and the volume is mounted in it.

- Recreating the pod

  When a pod is deleted and recreated, it is always scheduled on the same node where its local persistent volume is bound, ensuring consistent access to the attached storage.
